{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Demonstration:\n",
    "This notebook showcases how to evaluate our GMF and NCF models. This notebook will go over the preprocessing steps as well as the evaluation steps.\n",
    "\n",
    "## Import Packages and Modules:\n",
    "These are the following packages and modules we'll be using for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Python packages.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## Import the preprocessing and evaluating modules.\n",
    "import preprocess\n",
    "import evaluation\n",
    "\n",
    "## Import the models.\n",
    "import models.gmf_model as GMF\n",
    "import models.ncf_model as NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset:\n",
    "Similar to the preprocessing steps in the `training_demo.ipynb` notebook, we'll have to assign numerical ID values for each unique user and item. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the MSD interaction csv file.\n",
    "interaction_df = pd.read_csv('interactions.csv')\n",
    "## Convert whether the user liked a song into binary (0 or 1).\n",
    "interaction_df['liked'] = (interaction_df['count'] > 0).astype(int)\n",
    "\n",
    "## Map the DataFrame with numerical IDs.\n",
    "mapped_df = preprocess.MapUserItemID(df = interaction_df)\n",
    "\n",
    "## Split the training and testing dataset using the leave-one-out technique.\n",
    "train_df, test_df = preprocess.LeaveOneOut(df = mapped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a set that contains all the positive interactions of users as well as all the unique item IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 404103/404103 [04:38<00:00, 1453.11it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create the corresponding sets to positive interactions and unique item IDs.\n",
    "user_positive_itemsets, item_pool = preprocess.CreatePositiveInteractions(df = mapped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Process:\n",
    "While we can use the `test_df` generated from the leave-one-out split, we have already created 10,000 randomly selected testing interactions. This is labeled as the `sampled_test_data.csv` file. You can load this in using `pandas` and append the user-item interactions as tuples into the `test_data` list. This process is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the testing interactions.\n",
    "test_df = pd.read_csv('sampled_test_data.csv')\n",
    "\n",
    "## Create a list of testing interactions.\n",
    "test_data = []\n",
    "\n",
    "## For each row in the test DataFrame, add the user and song as a tuple.\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    song_id = row['song_id']\n",
    "\n",
    "    ## Append tuple to the test_data list.\n",
    "    test_data.append((user_id, song_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now initialize the model and load in trained weights. The `ncf_msd_weights.h5` are saved weights from a NCF model that was trained over 20 epochs on the MSD dataset. Before loading in the weights, we'll need to pass in dummy inputs through the model so that it initializes its weights. Then, we can replace the weights with our trained weights. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the NCF model.\n",
    "ncf_model = NCF.NCFModel(\n",
    "                num_users = mapped_df['user_id'].nunique(), \n",
    "                num_items = mapped_df['song_id'].nunique(), \n",
    "                num_latent = 8\n",
    "            )\n",
    "\n",
    "## Pass dummy inputs to initialize the weights in the NCF model.\n",
    "ncf_model(np.array([np.array([0]), np.array([1])]))\n",
    "## After intializing the weights to the model, we can then load in the \n",
    "## trained weights to the model.\n",
    "ncf_model.load_weights('ncf_msd_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally evaluate the performance of the model using the `EvaluateModel` function. This function computes the Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) for each user in our testing dataset. \n",
    "\n",
    "The Hit Ratio calculates the probability that a liked song is within the user's top k recommendation list. The NDCG is similar to the HR metric, instead in incorporates the ranking of the liked song as well. \n",
    "\n",
    "Users can change the value of the amount of recommendations the model makes. Furthermore, users can use various sample numbers to see how the accuracy progresses over different parameters. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 100 Negative Test Samples: 100%|██████████| 10000/10000 [00:01<00:00, 5576.19it/s]\n",
      "Applying Model: 100%|██████████| 10000/10000 [04:55<00:00, 33.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio at Top 10: 0.8018 | NDCG at Top 10: 0.5482748504832984\n"
     ]
    }
   ],
   "source": [
    "## The top k recommendations for evaluating the performance of the models.\n",
    "k_val = 10\n",
    "\n",
    "## Compute the average Hit Ratio and Normalized Discounted Cumulative Gain (NDCG) using the EvaluateModel function.\n",
    "hit_ratio, ndcg = evaluation.EvaluateModel(model = ncf_model, \n",
    "                                           test_data = test_data, \n",
    "                                           user_positive_itemsets = user_positive_itemsets, \n",
    "                                           item_pool = item_pool, \n",
    "                                           num_negatives = 100, \n",
    "                                           top_k = k_val\n",
    "                                        )\n",
    "\n",
    "## Print the computed performance metrics.\n",
    "print(f'Hit Ratio at Top {k_val}: {hit_ratio} | NDCG at Top {k_val}: {ndcg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
