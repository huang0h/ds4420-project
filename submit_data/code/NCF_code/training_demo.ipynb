{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Demonstration:\n",
    "This notebook showcases how to train our GMF and NCF models. This notebook will go over the preprocessing steps as well as the training steps.\n",
    "\n",
    "## Import Packages and Modules:\n",
    "These are the following packages and modules we'll be using for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Python packages.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## Import the preprocessing and evaluating modules.\n",
    "import preprocess\n",
    "import training \n",
    "\n",
    "## Import the models.\n",
    "import models.gmf_model as GMF\n",
    "import models.ncf_model as NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset:\n",
    "First, we need to load in the interactions dataset. The Million Song Dataset (MSD) was converted into an interactions dataset and saved as `interactions.csv`, and we can load it in using `pandas`. We'll then convert each user-item interaction to binary (0 for negative interaction and 1 for positive interaction). \n",
    "\n",
    "Since the original MSD dataset has string valued IDs for both users and songs, we'll need to convert them to numerical values so that we can properly train our models. This is done using the `MapUserItemID` function.\n",
    "\n",
    "Finally, since we're working with a very sparse dataset, the standard training-validation split wasn't optimal. Instead, we used the leave-one-out technique, where a random positive interaction was left out and stored in the testing dataset. This is done by using the `LeaveOneOut` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the MSD interaction csv file.\n",
    "interaction_df = pd.read_csv('interactions.csv')\n",
    "## Convert whether the user liked a song into binary (0 or 1).\n",
    "interaction_df['liked'] = (interaction_df['count'] > 0).astype(int)\n",
    "\n",
    "## Map the DataFrame with numerical IDs.\n",
    "mapped_df = preprocess.MapUserItemID(df = interaction_df)\n",
    "\n",
    "## Split the training and testing dataset using the leave-one-out technique.\n",
    "train_df, test_df = preprocess.LeaveOneOut(df = mapped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also converted the interactions as `set`. This allowed us to sample random interactions more efficiently compared to other storage methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 404103/404103 [02:35<00:00, 2605.83it/s]\n"
     ]
    }
   ],
   "source": [
    "## Create the corresponding sets to positive interactions and unique item IDs.\n",
    "user_positive_itemsets, item_pool = preprocess.CreatePositiveInteractions(df = mapped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization:\n",
    "We can now load in our model. For this demonstration, we used the General Matrix Factorization (GMF) model as it was less complex than the Neural Collaborative Filtering (NCF) model. Hence, it trained at a faster speed. We used the Adam optimizer with the initial learning rate of $1e-4$ and the binary cross entropy loss function. This process is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the GMF model.\n",
    "gmf_model = GMF.GMFModel(\n",
    "                num_users = mapped_df['user_id'].nunique(), \n",
    "                num_items = mapped_df['song_id'].nunique(), \n",
    "                num_latent = 8\n",
    "            )\n",
    "\n",
    "## Compile the GMF model using the following optimizer, loss, and metrics.\n",
    "gmf_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "gmf_model.compile(optimizer = gmf_optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "Since we're working with a very sparse interaction dataset, we couldn't use all the interactions (positive and negative) when training our model. For example, let's say that user A has 5 interactions. There are 3,419 possible interactions user A can have with the amount of songs in the dataset. Hence, if we input 5 positive interactions and 3,415 negative interactions, it's very likely to skew the model. Therefore, we used the `GenerateTrainingData` function, which randomly selects a certain number of random negative interactions from the dataset. In the case of user A, we would choose 4 $\\times$ 5 negative interaction samples, giving us a total of 25 interactions for the training input. This way, we're able to avoid creating any biases when training the model.\n",
    "\n",
    "We used a batch size of 256 for this demonstration. Furthermore, we trained the GMF model over 5 epochs. However, we found out that the model's learning saturates around the 20th epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples:   0%|          | 0/162534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples: 100%|██████████| 162534/162534 [00:05<00:00, 28775.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "9494/9494 [==============================] - 25s 2ms/step - loss: 0.5927 - accuracy: 0.7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples: 100%|██████████| 162534/162534 [00:05<00:00, 28002.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "9494/9494 [==============================] - 23s 2ms/step - loss: 0.5065 - accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples: 100%|██████████| 162534/162534 [00:05<00:00, 29169.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "9494/9494 [==============================] - 23s 2ms/step - loss: 0.4968 - accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples: 100%|██████████| 162534/162534 [00:05<00:00, 27481.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "9494/9494 [==============================] - 23s 2ms/step - loss: 0.4693 - accuracy: 0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Negative Samples: 100%|██████████| 162534/162534 [00:05<00:00, 28848.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "9494/9494 [==============================] - 23s 2ms/step - loss: 0.3945 - accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "## Initialize a dictionary to keep track of the training loss and accuracy.\n",
    "total_history = {}\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    ## Train the model over the given number of epochs.\n",
    "    for epoch, (inputs, labels) in enumerate(training.GenerateTrainingData(train_df, user_positive_itemsets, item_pool, epochs = 5, n_neg_multiplier = 4)):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        ## Get the individual users and items.\n",
    "        users = inputs[:, 0]\n",
    "        items = inputs[:, 1]\n",
    "\n",
    "        ## Fit the GMF model and append to history.\n",
    "        history = gmf_model.fit(x = [users, items], y = labels, batch_size = 256, epochs = 1, verbose = 1)\n",
    "\n",
    "        ## Store the training loss and accuracy to our custom total_history dictionary.\n",
    "        for key, values in history.history.items():\n",
    "            if key not in total_history:\n",
    "                total_history[key] = [] \n",
    "            total_history[key].extend(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Model:\n",
    "Using the trained model, we can now use the `predict` function to predict the interaction score given a random user and item. This process is shown below.\n",
    "\n",
    "**NOTE:** Since the model has only been trained for 5 epochs, the performance may be very poor. Please train the model over approximately 20 - 25 epochs for optimal perforamnce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Interaction: 0.2119390368461609\n"
     ]
    }
   ],
   "source": [
    "## Choose a random index to evaluate model on.\n",
    "rand_index = random.choice(mapped_df.index)\n",
    "\n",
    "## Extract the users and songs from this row.\n",
    "user_test = np.array([mapped_df.iloc[rand_index]['user_id']])\n",
    "song_test = np.array([mapped_df.iloc[rand_index]['song_id']])\n",
    "\n",
    "## Create interaction array for both user and song.\n",
    "user_song = [user_test, song_test]\n",
    "\n",
    "## Input the interaction into the model to get a predicted interaction score.\n",
    "pred_score = gmf_model.predict(user_song, verbose = 0).item()\n",
    "print(f'Predicted Interaction: {pred_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
