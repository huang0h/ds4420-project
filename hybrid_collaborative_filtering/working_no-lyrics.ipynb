{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "from itertools import combinations\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../rawdata/msd_subset_audio_features.json', 'r') as f:\n",
    "    merged_features_raw = json.load(f)\n",
    "\n",
    "with open('../rawdata/msd_taste_profile_no-lyrics.json', 'r') as f:\n",
    "    taste_profile_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 404103)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_features_raw), len(taste_profile_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'duration': 1819.76771,\n",
       "  'key': 11.0,\n",
       "  'key_confidence': 1.0,\n",
       "  'mode': 1.0,\n",
       "  'mode_confidence': 1.0,\n",
       "  'loudness': 0.566,\n",
       "  'tempo': 262.828,\n",
       "  'time_signature': 7.0,\n",
       "  'time_signature_confidence': 1.0},\n",
       " {'duration': 1.04444,\n",
       "  'key': 0.0,\n",
       "  'key_confidence': 0.0,\n",
       "  'mode': 0.0,\n",
       "  'mode_confidence': 0.0,\n",
       "  'loudness': -51.643,\n",
       "  'tempo': 0.0,\n",
       "  'time_signature': 0.0,\n",
       "  'time_signature_confidence': 0.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize content features to [0, 1]\n",
    "max_features = {\n",
    "    'duration': -np.inf,\n",
    "    'key': -np.inf,\n",
    "    'key_confidence': -np.inf,\n",
    "    'mode': -np.inf,\n",
    "    'mode_confidence': -np.inf,\n",
    "    'loudness': -np.inf,\n",
    "    'tempo': -np.inf,\n",
    "    'time_signature': -np.inf,\n",
    "    'time_signature_confidence': -np.inf,\n",
    "}\n",
    "\n",
    "min_features = {\n",
    "    'duration': np.inf,\n",
    "    'key': np.inf,\n",
    "    'key_confidence': np.inf,\n",
    "    'mode': np.inf,\n",
    "    'mode_confidence': np.inf,\n",
    "    'loudness': np.inf,\n",
    "    'tempo': np.inf,\n",
    "    'time_signature': np.inf,\n",
    "    'time_signature_confidence': np.inf,\n",
    "}\n",
    "\n",
    "for _, features in merged_features_raw.items():\n",
    "    for feature, value in features.items():\n",
    "        if feature in max_features:\n",
    "            max_features[feature] = max(max_features[feature], value)\n",
    "            min_features[feature] = min(min_features[feature], value)\n",
    "            \n",
    "max_features, min_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    # Filter out unrecognized features\n",
    "    feature_subset = { feature_name: features[feature_name] for feature_name in max_features }\n",
    "    \n",
    "    # Normalize recognized features\n",
    "    for feature, value in feature_subset.items():\n",
    "        if feature in max_features:\n",
    "            feature_subset[feature] = (value - min_features[feature]) / (max_features[feature] - min_features[feature])\n",
    "            \n",
    "    return feature_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_features_processed = { track_id: process_features(features) for track_id, features in merged_features_raw.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles, user_setlists = generate_user_profile(taste_profile_raw, merged_features_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_user_profiles = {}\n",
    "user_setlists = {}\n",
    "\n",
    "for user_id, tracklist in taste_profile_raw.items():\n",
    "    initial = {\n",
    "        'duration': 0,\n",
    "        'key': 0,\n",
    "        'key_confidence': 0,\n",
    "        'mode': 0,\n",
    "        'mode_confidence': 0,\n",
    "        'loudness': 0,\n",
    "        'tempo': 0,\n",
    "        'time_signature': 0,\n",
    "        'time_signature_confidence': 0,\n",
    "    }\n",
    "    user_setlists[user_id] = {}\n",
    "    \n",
    "    # Generate weighted average of features\n",
    "    total_track_counts = 0\n",
    "    \n",
    "    for track in tracklist:\n",
    "        track_id, count = track['track'], track['count']\n",
    "        if track_id not in merged_features_processed:\n",
    "            raise 'AAAAA'\n",
    "        \n",
    "        total_track_counts += count\n",
    "        user_setlists[user_id][track_id] = count\n",
    "        \n",
    "        track_features = merged_features_processed[track_id]\n",
    "        for feature, value in track_features.items():\n",
    "            initial[feature] += value * count\n",
    "            \n",
    "    for feature in initial:\n",
    "        initial[feature] /= total_track_counts\n",
    "    \n",
    "    average_user_profiles[user_id] = initial\n",
    "\n",
    "len(average_user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_euclidean_distance(a, b):\n",
    "    return np.linalg.norm(np.array(list(a.values())) - np.array(list(b.values())))\n",
    "\n",
    "def dict_manhtattan_distance(a, b):\n",
    "    return np.sum(np.abs(np.array(list(a.values())) - np.array(list(b.values()))))\n",
    "\n",
    "# Define a user's rating of a track as the number of times they listened to it\n",
    "# If they haven't listened to it, define rating as the similarity (i.e. inverse distance) between the user's average profile and the track's features\n",
    "def user_rating(user_id, track_id, distance_func: callable=dict_euclidean_distance):\n",
    "    if track_id in user_setlists[user_id]:\n",
    "        return user_setlists[user_id][track_id]\n",
    "    else:\n",
    "        user_avg_features = average_user_profiles[user_id]\n",
    "        track_features = merged_features_processed[track_id]\n",
    "        \n",
    "        return 1 / (1 + distance_func(user_avg_features, track_features))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_user_sim(user_id1, user_id2):\n",
    "    combined_songs = set([*user_setlists[user_id1].keys(), *user_setlists[user_id2].keys()])\n",
    "    n_songs = len(combined_songs)\n",
    "    \n",
    "    user1_ratings = np.zeros(n_songs)\n",
    "    user2_ratings = np.zeros(n_songs)\n",
    "    \n",
    "    for i, song in enumerate(combined_songs):\n",
    "        user1_ratings[i] = user_rating(user_id1, song)\n",
    "        user2_ratings[i] = user_rating(user_id2, song)\n",
    "    \n",
    "    user1_ratings[:] -= np.mean(user1_ratings)\n",
    "    user2_ratings[:] -= np.mean(user2_ratings)\n",
    "    \n",
    "    num = np.dot(user1_ratings, user2_ratings)\n",
    "    den = np.linalg.norm(user1_ratings) * np.linalg.norm(user2_ratings)\n",
    "    \n",
    "    if num == 0 and den == 0:\n",
    "        return 1\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def cosine_user_sim(user_id1, user_id2):\n",
    "    combined_songs = set([*user_setlists[user_id1].keys(), *user_setlists[user_id2].keys()])\n",
    "    n_songs = len(combined_songs)\n",
    "    \n",
    "    user1_ratings = np.zeros(n_songs)\n",
    "    user2_ratings = np.zeros(n_songs)\n",
    "    \n",
    "    for i, song in enumerate(combined_songs):\n",
    "        user1_ratings[i] = user_rating(user_id1, song)\n",
    "        user2_ratings[i] = user_rating(user_id2, song)\n",
    "    \n",
    "    num = np.dot(user1_ratings, user2_ratings)\n",
    "    den = np.linalg.norm(user1_ratings) * np.linalg.norm(user2_ratings)\n",
    "    \n",
    "    if num == 0 and den == 0:\n",
    "        return 0\n",
    "    \n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate user ID to index mapping\n",
    "userid_to_index = { user_id: i for i, user_id in enumerate(average_user_profiles.keys()) }\n",
    "index_to_userid = { v: k for k, v in userid_to_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 221\n",
    "rand = np.random.default_rng(RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random user subset\n",
    "subset_size = 5000\n",
    "user_subset = rand.choice(len(average_user_profiles), subset_size, replace=False)\n",
    "index_subset_mapping = { i: index_to_userid[user_id] for i, user_id in enumerate(user_subset) }\n",
    "subset_index_mapping = { v: k for k, v in index_subset_mapping.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=12495000 / 12497500, 99.980%\r"
     ]
    }
   ],
   "source": [
    "user_similarity = np.full((subset_size, subset_size), -np.inf)\n",
    "total_sims = (subset_size * (subset_size - 1)) // 2\n",
    "\n",
    "for i, pair in enumerate(combinations(index_subset_mapping, 2)):\n",
    "    if i % 2500 == 0:\n",
    "        print(f'{i=}/{total_sims} | {(i/total_sims * 100):.3f}%', end='\\r')\n",
    "    \n",
    "    user1, user2 = pair\n",
    "    sim = cosine_user_sim(index_subset_mapping[user1], index_subset_mapping[user2])\n",
    "    \n",
    "    user_similarity[user1][user2] = sim\n",
    "    user_similarity[user2][user1] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neighborhood_analysis(neighborhood_size):\n",
    "    user_neighborhoods = {}\n",
    "    user_neighborhoods[user]\n",
    "    \n",
    "    # for i, user in enumerate(user_subset):\n",
    "    #     if i % 250 == 0:\n",
    "    #         print(f'{i=}/{subset_size}', end='\\r')\n",
    "        \n",
    "    #     user_neighborhoods[user] = np.argsort(user_similarity[i])[-neighborhood_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigbhorhood_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_neighborhood_analysis.<locals>.predict_rating() missing 1 required positional argument: 'neighborhoods'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m neighborhood_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mrun_neighborhood_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mneighborhood_sizes\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m neighborhood_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m errors \u001b[38;5;241m=\u001b[39m [\u001b[43mrun_neighborhood_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m neighborhood_sizes]\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mrun_neighborhood_analysis\u001b[0;34m(neighborhood_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m taste_profile_raw[user]:\n\u001b[1;32m     24\u001b[0m     track_id \u001b[38;5;241m=\u001b[39m track[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     actual \u001b[38;5;241m=\u001b[39m track[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(f'Prediction on track {track_id}: {prediction} | Actual: {actual}')\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: run_neighborhood_analysis.<locals>.predict_rating() missing 1 required positional argument: 'neighborhoods'"
     ]
    }
   ],
   "source": [
    "neighborhood_sizes = [3, 5, 10, 25, 50, 100, 250, 500, 1000]\n",
    "errors = [run_neighborhood_analysis(size) for size in neighborhood_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 29.72595568405758\n"
     ]
    }
   ],
   "source": [
    "run_count = 0\n",
    "error = 0\n",
    "for user in user_neighborhoods:\n",
    "    # print(f'User {user}:')\n",
    "    for track in taste_profile_raw[user]:\n",
    "        track_id = track['track']\n",
    "        \n",
    "        prediction = predict_rating(user, track_id)\n",
    "        actual = track['count']\n",
    "        \n",
    "        # print(f'Prediction on track {track_id}: {prediction} | Actual: {actual}')\n",
    "        \n",
    "        error += (actual - prediction) ** 2\n",
    "        run_count += 1\n",
    "        \n",
    "print(f'MSE: {error / run_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:\n",
    "- training process:\n",
    "  - for each of user's songs:\n",
    "    - generate recommendations using other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combining both hybrid feature and interaction models:\n",
    "- convert msd taste profile into piki-like interactions\n",
    "  - run interaction model using said data\n",
    "  - run feature model similarly\n",
    "  - weighted average of both models' outputs\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
